{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "bookNames = nltk.corpus.gutenberg.fileids()\n",
    "print(bookNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "books = [nltk.corpus.gutenberg.words(bookName) for bookName in bookNames]\n",
    "print(len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'and',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " ',',\n",
       " 'seemed']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(book):\n",
    "    #1. Remove non-alphanumeric words & convert to lowercase\n",
    "    cleanBook = [word.lower() for word in book if word.isalpha() and len(word) > 2]\n",
    "    #2. Remove stopwords\n",
    "    cleanBook = [word for word in cleanBook if word not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    #2. Stemming and Lemmetization\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    cleanBook = [stemmer.stem(WordNetLemmatizer().lemmatize(word, pos='v')) for word in cleanBook if word not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    return cleanBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = preprocess(books[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'emma': 865, 'mrs': 699, 'think': 692, 'miss': 611, 'say': 566, 'know': 541, 'harriet': 506, 'thing': 460, 'weston': 448, 'elton': 407, ...})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanBooks = [preprocess(book) for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "# In simple words a dictionary is nothing but unique words and their indexes stored together\n",
    "booksDictionary = gensim.corpora.Dictionary(cleanBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(24410 unique tokens: ['abbey', 'abbot', 'abdi', 'abhor', 'abid']...)\n"
     ]
    }
   ],
   "source": [
    "print(booksDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbey\n",
      "abbot\n",
      "abdi\n",
      "abhor\n",
      "abid\n",
      "abil\n",
      "abl\n",
      "abolit\n",
      "abomin\n",
      "abroad\n"
     ]
    }
   ],
   "source": [
    "#Print some values from the dictionary\n",
    "values = [print(v) for k, v in booksDictionary.iteritems() if k < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove words that don't appear in at least 2 books (remove words that are TOO specific)\n",
    "# 2. Keep words that appear in not more than 50% of the books (remove words that are TOO common)\n",
    "# 3. After #1 and #2, keep the first 10k words\n",
    "# filter_extremes documentation: https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes\n",
    "booksDictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abrupt\n"
     ]
    }
   ],
   "source": [
    "print(booksDictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (5, 3),\n",
       " (7, 6),\n",
       " (8, 13),\n",
       " (9, 4),\n",
       " (10, 1),\n",
       " (12, 5),\n",
       " (14, 3)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create a Bag-of-Word corpus\n",
    "# A BoW corpus replaces the words in a document with their corresponding index in dictionary \n",
    "# and the number of times it appears in the document\n",
    "# For e.g., [\"Who let the dogs out?\", \"Who? Who? Who? Who?\"]\n",
    "# The BoW for the above would be something like: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n",
    "# Where [4, 1] represents that the word is Who and appeared once in first sentence and\n",
    "# [4, 4] represents that Who appeared 4 times in the sentence\n",
    "# Once we create the BoW, the semantics and word postions go out the window.\n",
    "cleanBooks_BoW_Corpus = [booksDictionary.doc2bow(book) for book in cleanBooks]\n",
    "cleanBooks_BoW_Corpus[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abhor, 3\n",
      "abomin, 1\n",
      "abrupt, 4\n",
      "abund, 3\n",
      "accident, 6\n",
      "accommod, 13\n",
      "accomplish, 4\n",
      "accost, 1\n",
      "ach, 5\n",
      "acquiesc, 3\n"
     ]
    }
   ],
   "source": [
    "# To see replace BoW indexes with words, we can do this:\n",
    "word_indexes = [print(booksDictionary[k] + ', ' + str(v)) for k,v in cleanBooks_BoW_Corpus[2][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA Model\n",
    "# Pass LAST 17 books (out of a total of 18)\n",
    "# The last book would be for testing the generated model\n",
    "lda_model = gensim.models.LdaMulticore(cleanBooks_BoW_Corpus[1:18], num_topics=10, id2word=booksDictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.025*\"mrs\" + 0.020*\"edward\" + 0.013*\"big\" + 0.012*\"colonel\" + 0.011*\"farmer\" + 0.010*\"mous\" + 0.008*\"rabbit\" + 0.007*\"unto\" + 0.006*\"turtl\" + 0.006*\"couldn\"\n",
      "Topic: 1 \n",
      "Words: 0.159*\"unto\" + 0.042*\"israel\" + 0.034*\"hath\" + 0.030*\"shalt\" + 0.018*\"thine\" + 0.016*\"jesus\" + 0.014*\"thereof\" + 0.012*\"mose\" + 0.011*\"egypt\" + 0.011*\"spake\"\n",
      "Topic: 2 \n",
      "Words: 0.025*\"unto\" + 0.023*\"hath\" + 0.016*\"whale\" + 0.008*\"israel\" + 0.007*\"shalt\" + 0.006*\"thine\" + 0.006*\"boat\" + 0.005*\"spake\" + 0.005*\"adam\" + 0.004*\"eve\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"mrs\" + 0.012*\"guinea\" + 0.011*\"arthur\" + 0.007*\"whilst\" + 0.007*\"hath\" + 0.006*\"farmer\" + 0.006*\"price\" + 0.006*\"adam\" + 0.005*\"eve\" + 0.005*\"hardi\"\n",
      "Topic: 4 \n",
      "Words: 0.008*\"professor\" + 0.005*\"whale\" + 0.005*\"bull\" + 0.004*\"presid\" + 0.004*\"comrad\" + 0.004*\"boat\" + 0.004*\"colonel\" + 0.004*\"vast\" + 0.003*\"mrs\" + 0.003*\"big\"\n",
      "Topic: 5 \n",
      "Words: 0.042*\"ham\" + 0.028*\"caesar\" + 0.014*\"hath\" + 0.013*\"hamlet\" + 0.012*\"unto\" + 0.008*\"doth\" + 0.006*\"shew\" + 0.006*\"thine\" + 0.006*\"ant\" + 0.005*\"shalt\"\n",
      "Topic: 6 \n",
      "Words: 0.069*\"unto\" + 0.024*\"israel\" + 0.022*\"hath\" + 0.017*\"shalt\" + 0.016*\"mrs\" + 0.011*\"thine\" + 0.010*\"thereof\" + 0.009*\"jesus\" + 0.008*\"jerusalem\" + 0.008*\"mose\"\n",
      "Topic: 7 \n",
      "Words: 0.058*\"whale\" + 0.024*\"mrs\" + 0.020*\"ann\" + 0.017*\"boat\" + 0.008*\"chapter\" + 0.007*\"deck\" + 0.006*\"crew\" + 0.005*\"aye\" + 0.005*\"mast\" + 0.005*\"sailor\"\n",
      "Topic: 8 \n",
      "Words: 0.012*\"unto\" + 0.008*\"whale\" + 0.007*\"hath\" + 0.005*\"israel\" + 0.005*\"mrs\" + 0.005*\"big\" + 0.004*\"boat\" + 0.004*\"shalt\" + 0.003*\"abrupt\" + 0.003*\"michael\"\n",
      "Topic: 9 \n",
      "Words: 0.169*\"unto\" + 0.048*\"israel\" + 0.044*\"hath\" + 0.029*\"shalt\" + 0.018*\"jesus\" + 0.017*\"thereof\" + 0.017*\"mose\" + 0.016*\"thine\" + 0.016*\"jerusalem\" + 0.011*\"brethren\"\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(i, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7590150237083435\t Topic: 0.058*\"whale\" + 0.024*\"mrs\" + 0.020*\"ann\" + 0.017*\"boat\" + 0.008*\"chapter\"\n",
      "Score: 0.12667332589626312\t Topic: 0.025*\"mrs\" + 0.020*\"edward\" + 0.013*\"big\" + 0.012*\"colonel\" + 0.011*\"farmer\"\n",
      "Score: 0.11383391171693802\t Topic: 0.021*\"mrs\" + 0.012*\"guinea\" + 0.011*\"arthur\" + 0.007*\"whilst\" + 0.007*\"hath\"\n"
     ]
    }
   ],
   "source": [
    "for i, score in sorted(lda_model[cleanBooks_BoW_Corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(i, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
